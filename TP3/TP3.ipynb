{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TP3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5fHWJhwOXGzh"},"source":["# INF8111 - Fouille de données / Data Mining\n","## Automne 2020 - TP3 - Fouille de réseaux sociaux / Mining of social networks\n","### Membres de l'équipe / Team members\n","    - Membre 1\n","    - Membre 2\n","    - Membre 3\n"]},{"cell_type":"markdown","metadata":{"id":"TmGvqtSVgfXi"},"source":["## Instructions de remise / Submission\n","\n","Vous devez remettre dans la boîte de remise sur moodle:\n","\n","1. ce fichier nommé TP3\\_NomDuMembre1\\_NomDuMembre2\\_NomDuMembre3.ipynb\n","\n","**N.B**: Assurez-vous que tous les résultats soient lisibles lorsque le notebook est ouvert.\n","\n","Tout devra être remis avant le **23 juin 2021 à 23h55**. Tout travail en retard sera pénalisé d’une valeur de 10\\% par jour ouvrable de retard.\n","\n","## Barème\n","\n","Partie 1: 14 points\n","\n","Partie 2: 6 points\n","\n","Pour un total de 20 points.\n","\n","\n","---\n","\n","## Submission\n","\n","You must put back in the submission box on moodle:\n","\n","1. this file renamed TP3\\_NomDuMembre1\\_NomDuMembre2\\_NomDuMembre3.ipynb\n","\n","**N.B**: Make sure that all results are there when you open your notebook.\n","\n","Everything must be submitted before **June 26th 2021 à 23h55**. Any late work will be penalized with a value of 10% per open day of delay.\n","\n","## Barème\n","Part 1: 14 points\n","\n","Part 2: 6 points\n","\n","For a total of 20 points.\n"]},{"cell_type":"markdown","metadata":{"id":"P1et8f3nXGzk"},"source":["## Réseaux sociaux / Social Networks\n","Les réseaux sociaux occupent une grande partie de la vie humaine. Chaque personne appartient tout le long de sa vie à différentes communautés. Avec le rassemblage de ces informations sur les différentes plateformes en ligne de réseaux sociaux, les analystes de données ont voulu exploiter ces données. C'est un domaine relativement nouveau qui est en pleine croissance avec plusieurs impacts sur plusieurs aspects tels que la publicité et les systèmes de recommandation. \n","\n","### But\n","Le but de ce TP est de vous donner un aperçu de l'analyse d'un réseau social.\n","\n","Dans la première partie, vous implémenterez un algorithme de détection de communautés dans un réseau social nommé LPAm+. Cet algorithme a été proposé par [X. Liu et T. Murata en 2010](https://www.sciencedirect.com/science/article/pii/S0378437109010152).\n","\n","Dans la deuxième partie, vous trouverez les personnes avec le plus d'influence dans leur réseau social. \n","\n","Pour les deux parties, nous vous fournissons les CSV contenant les réseaux sociaux à analyser.\n","\n","\n","---\n","\n","## Social networks\n","Social networks are a major component of the human life. Each person belongs throughout their life to different communities. With the aggregation of information on various online social media platforms, data analysts were interested in exploiting its data. It is a relatively new field that is growing with impacts on several aspects such as advertising and recommendation systems.\n","\n","\n","### Goal\n","The purpose of this lab is to give you an overview of social network analysis.\n","\n","In the first part, you will implement an algorithm for detecting communities in a social network called LPAm+. This algorithm was proposed by [X. Liu and T. Murata in 2010](https://www.sciencedirect.com/science/article/pii/S0378437109010152).\n","\n","In the second part, you will find the people with the most influence in their social network.\n","\n","For both parts, we provide you with the CSV containing the social networks to be analysed."]},{"cell_type":"code","metadata":{"id":"muc8hPSd1xet"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"517JwzlPXGzp"},"source":["# 1. LPAm+ (14 points)\n","\n","## Détection de communauté\n","La détection de communauté dans un réseau social est une manipulation fréquente lors de l'analyse d'un réseau. Une méthode de clustering est utilisée pour rassembler les personnes dans des communautés selon les liens entre eux. \n","\n","## LPAm+\n","Dans cette partie, vous implémenterez l'algorithme LPAm+ pour détecter les communautés parmi les personnages de Games of Thrones. Vous devez utiliser les CSV *nodes* et *edges* pour cela. \n","\n","Cet algorithme consiste à propager les étiquettes dans le réseau selon une règle d'évaluation optimisant la modularité du réseau. Lorsque l'algorithme atteint un optimum local, il regarde s'il peut combiner deux communautés pour augmenter la modularité du réseau. L'algorithme choisit toujours la combinaison la plus avantageuse. Si une combinaison est trouvée, la propagation des étiquettes est refaite. L'algorithme continue tant qu'elle peut améliorer la modularité. Vous pouvez lire l'article mentionné plus haut pour plus de détails, mais cela n'est pas nécessaire puisque vous allez être guidé tout le long du TP. \n","\n","Pour faciliter la représentation du réseau, nous vous proposons d'utiliser le package networkx. La documentation est disponible [ici](https://networkx.github.io/documentation/stable/tutorial.html).\n","\n","\n","\n","---\n","\n","# 1. LPAm+ (14 points)\n","\n","\n","## Community detection\n","Community detection in a social network is a frequent manipulation when analysing a network. A clustering method is used to bring people together in communities according to the links between them.\n","\n","\n","## LPAm+\n","In this part, you will implement the LPAm+ algorithm to detect the communities among the characters of Games of Thrones. You must use the nodes and edges csv for this.\n","\n","This algorithm consists in propagating the labels in the network according to an evaluation rule optimizing the modularity of the network. When the algorithm reaches a local optimum, it checks whether it can combine two communities to increase the modularity of the network. The algorithm always chooses the most advantageous combination. If a combination is found, the propagation of the labels is redone. The algorithm continues until it is no longer able to increase modularity. You can read the article mentioned above for more details, but you do not need to, as you will be guided throughout the TP.\n","\n","\n","To help you represent a network, we suggest that you use the networkx package.You can read more about the package [here](https://networkx.github.io/documentation/stable/tutorial.html)."]},{"cell_type":"code","metadata":{"id":"9DGyw323Srh0"},"source":["# vous pouvez bien sûr utiliser anaconda pour installer les packages\n","!pip install --user numpy\n","!pip install --user pandas\n","!pip install --user matplotlib\n","!pip install --user networkx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SK6OhgIoSrh5"},"source":["import numpy as np\n","import networkx as nx\n","import random\n","import itertools\n","import math\n","\n","\n","class LPAmPlus:\n","    \"\"\"\n","    Contructor\n","    \"\"\"\n","\n","    def __init__(self, graph):\n","        \"\"\"\n","        graph gives the graph on which the algorithm will be applied;\n","        \"\"\"\n","        self.graph = graph\n","\n","\n","        \"\"\"\n","        Assign a label to each node\n","        \"\"\"\n","        #TODO\n","\n","\n","        \"\"\"\n","        labels gives all the communities present in the network\n","        \"\"\"\n","        self.labels = None\n","\n","       \n","\n","    \"\"\"\n","    Term to optimize when replacing labels\n","    \"\"\"\n","\n","    def label_evaluation(self, current_node, new_label):\n","        #TODO\n","        return 0\n","\n","    \"\"\"\n","    Function to choose the new label for a node\n","    \"\"\"\n","\n","    def update_label(self, current_node):\n","        #TODO\n","        pass\n","\n","    \"\"\"\n","    Function that calculates the current modularity of the network\n","    \"\"\"\n","\n","    def modularity(self):\n","        #TODO\n","        return 0\n","    \n","    \n","    \"\"\"\n","    Function that applies the LPAm algorithm on the network\n","    \"\"\"\n","\n","    def LPAm(self):\n","        #TODO\n","        pass\n","        \n","    \"\"\"\n","    Function that find which communities to combine and combine them\n","    \"\"\"\n","    def merge_communities(self):\n","        #TODO\n","        return False\n","    \n","    \n","    \"\"\"\n","    Function that applies the LPAm+ algorithm on the network\n","    \"\"\"\n","\n","    def find_communities(self):\n","        #TODO\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yv83c1sWXGzq"},"source":["### 1.1 Dataset (1 point)\n","\n","Nous vous avons fourni les CSV pour toutes les saisons de Games of Thrones. Vous devez maintenant représenter ces réseaux en utilisant les deux CSV fournis pour chaque saison: un pour les sommets et un pour les arêtes. \n","\n","\n","#### Implémentation\n","1. Implémentez  la fonction  *`load_unweighted_network`*. Cette fonction retourne le réseau non dirigé et sans poids.\n","\n","Utilisez la fonction `test_load` pour vérifier votre implémentation de la fonction. Ce test utilise un petit toy dataset. Vous devriez avoir quelque chose de similaire (data/picture.png):\n","![title](data/picture.png)\n","\n","\n","---\n","We have provided you with the csv for all the seasons of Games of Thrones. You must now represent each of those networks in code using two csv for each season: the one for the nodes and the one for the edges.\n","\n","\n","#### Implementation\n","1. Implement the function *`load_unweighted_network`*. This function returns a undirected and unweighted graph.\n","\n","Use the function `test_load` to verify your implementation of the function. This test use a toy dataset. You should obtain a result similar to this (data/picture.png):\n","![title](data/picture.png)\n"]},{"cell_type":"code","metadata":{"id":"1wGC7MC3Srh-"},"source":["import csv\n","import pandas as pd\n","import networkx as nx\n","\n","\n","def load_unweighted_network(node_csv, edge_csv):\n","    #TODO\n","    \n","    return network"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HEwBsNVsjvfs"},"source":["import matplotlib.pyplot as plt\n","def test_load():\n","    network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n","    nx.draw_networkx(network,font_color='white')\n","    plt.show()\n","\n","test_load()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6UTIxaGIXGzz"},"source":["### 1.2  Modularité / Modularity (2 point)\n","\n","La modularité $Q$ du réseau est une mesure importante pour l'algorithme: elle permet de savoir si l'algorithme a atteint un optimum local. $$ Q=\\frac{1}{2m}\\sum_{u,v=1}^n B_{uv}\\delta(l_u,l_v)$$ \n","\n","- m: le nombre d'arêtes\n","- l: l'étiquette du sommet\n","- u, v: des sommets dans le réseau\n","- B: la matrice de modularité où chaque élément vaut $A_{uv} - P_{uv}$\n","- $A_{uv}$: vaut 1 si il y une arête entre u et v sinon 0\n","- $P_{uv}$: la probabilité qu'il y ait une arête entre u et v selon le modèle nul  $$P_{uv}=\\frac{degree(u)*degree(x)}{2m}$$\n","- $\\delta(l_u,l_v)$: delta de Kronecker, vaut 1 si les deux labels sont identiques sinon 0\n","\n","#### Implémentation\n","1. Implémentez  la fonction  `modularity`  dans LPAmPlus. Cette fonction retourne la modularité du réseau. Vous pouvez utiliser la fonction `nx.linalg.modularity_matrix` de networkx pour calculer la matrice B. **N.B:** Networkx permet d'ajouter du data sur les sommets pour garder des informations sur le node. Les nodes agissent comme des dictionnaires.\n","\n","Utilisez la fonction `test_modularity` pour vérifier votre implémentation de la fonction. Vous devriez obtenir une modularité d'environ 0.413.\n","\n","---\n","\n","The modularity $Q$ of the network is an important measure for the algorithm. The algorithm uses it to determine if it reached a local optimum or not. $$ Q=\\frac{1}{2m}\\sum_{u,v=1}^n B_{uv}\\delta(l_u,l_v)$$ \n","\n","- m: number of edges\n","- l: node's label\n","- u, v: nodes in the graph\n","- B: modularity matrix where each element is $A_{uv} - P_{uv}$\n","- $A_{uv}$: is 1 if there is an edge between u and v else 0\n","- $P_{uv}$: probability that there is an edge between u and v following the null model $$P_{uv}=\\frac{degree(u)*degree(x)}{2m}$$\n","- $\\delta(l_u,l_v)$: Kronecker's delta, is 1 if labels are the same else 0\n","\n","#### Implementation\n","1. Implement the function `modularity` in the class LPAmPlus. This function returns the modularity of the network. You can use the function `nx.linalg.modularity_matrix` from networkx to calculate B. **N.B:** You can add data to nodes with Networkx to store information about the node. You can add data to nodes with Networkx to store information about the node. The nodes act like a dictionnary.\n","\n","Use the function `test_modularity` to test your implementation. You should have a modularity of 0.413."]},{"cell_type":"markdown","metadata":{"id":"t8iBYh5ohman"},"source":[""]},{"cell_type":"code","metadata":{"id":"B7szMoEQSriF"},"source":["def test_modularity():\n","    social_network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n","    lpam = LPAmPlus(social_network)\n","    lpam.labels = [0, 1]\n","    for i in [0,1,2,3,4,5,6,7,8,9]:\n","        lpam.graph.nodes[i]['label'] = 0\n","    for i in [10,11,12,13,14,15]:\n","        lpam.graph.nodes[i]['label'] = 1\n","    print(\"Modularity: {:.3f}\".format(lpam.modularity()))\n","\n","test_modularity()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vwxvqYj4XGzu"},"source":["### 1.3 Règle de modification des étiquettes / Updating rule for the labels (3 point)\n","\n","Comme mentionné plus haut, l'algorithme est fortement basé sur son optimisation de la modularité. Il vous est maintenant demandé d'implémenter le terme à optimiser. La nouvelle étiquette $l_x^{new}$ correspond à l'étiquette pour laquelle la somme donne la plus grande valeur.\n","$$l_x^{new}=\\arg\\max_l\\sum_{u=1}^n B_{ux}\\delta(l_u,l)$$\n","\n","- n: le nombre de sommets\n","- m: le nombre d'arêtes\n","- l: une étiquette possible pour le sommet x\n","- x: le sommet qu'on évalue en ce moment\n","- u: un autre sommet dans le réseau (commence à 1, car on exclut le sommet x)\n","- B: la matrice de modularité où chaque élément vaut $A_{ux} - P_{ux}$\n","- $A_{ux}$: vaut 1 si il y une arête entre u et x sinon 0\n","- $P_{ux}$: la probabilité qu'il y ait une arête entre u et x selon le modèle nul  $$P_{ux}=\\frac{degree(u)*degree(x)}{2m}$$\n","- $\\delta(l_u,l)$: delta de Kronecker, vaut 1 si les deux labels sont identiques sinon 0\n","\n","\n","#### Implémentation\n","1. Implémenter la fonction `label_evaluation`. Cette fonction retourne la valeur du terme à optimiser. Vous pouvez utiliser la fonction `linalg.modularity_matrix` de networkx pour calculer la matrice B. Il est normal qu'il y ait une ressemblance avec le calcul de la modularité selon la définition que vous avez prise. `new_label` correspond donc à un $l$ possible dans le terme.\n","2. Implémenter la fonction `update_label`. Cette fonction choisit la nouvelle étiquette pour le sommet actuel. En cas d'égalité, la fonction choisit une étiquette au hasard parmi les meilleurs. N'oubliez pas d'enlever les étiquettes désuètes du paramètre `labels`. **N.B:** Il est possible que la meilleure étiquette soit celle actuelle du sommet.\n","\n","Networkx permet d'ajouter du data sur les sommets. Les sommets sont des dictionnaires dans le graphe.\n","\n","---\n","\n","As mentioned above, the algorithm is strongly based on its optimization of modularity. You are now asked to implement the term to optimize. The new label $l_x^{new}$ corresponds to the label for which the sum gives the greatest value.\n","$$l_x^{new}=\\arg\\max_l\\sum_{u=1}^n B_{ux}\\delta(l_u,l)$$\n","\n","- n: number of nodes\n","- m: number of edges\n","- l: a possible label for the node x\n","- x: current node being evaluated\n","- u: another node in the network (starts at 1, because we exclude the node x)\n","- B: modularity matrix where each element is $A_{ux} - P_{ux}$\n","- $A_{ux}$: is 1 if there is an edge between u and x else 0\n","- $P_{ux}$: the probability that there is an edge between u and x  following the null model  $$P_{ux}=\\frac{degree(u)*degree(x)}{2m}$$\n","- $\\delta(l_u,l)$: Kronecker's delta, is 1 if labels are the same else 0\n","\n","\n","#### Implementation\n","1. Implement the function `label_evaluation`. This function returns the value for the term to optimize. You can use the function `linalg.modularity_matrix` from networkx to calculate B. It is normal if there is a similarity with the modularity depending on the definition you took. `new_label` represent a possible $l$ in the term.\n","2. Implement the function `update_label`. This function chooses the new label for the current node. If there is more than one label with the max value, the function chooses randomly one amoung those. Don't forget to remove the unused labels from the `labels` attribute. **N.B:** The best label can be the node's current label. \n","\n","You can add data to nodes with Networkx to store information about the node. The nodes act like a dictionnary.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"g5_N31uPXGz2"},"source":["### 1.4 LPAm (2 points)\n","\n","Vous pouvez maintenant implémenter l'algorithme LPAm. Cet algorithme est le prédécesseur de LPAm+ puisque LPAm+ a été crée pour contourner une faiblesse de LPAm.  LPAm est un algorithme de propapagation d'étiquettes basé sur la modularité. Il commence par donner une étiquette unique à chaque sommet. Il explore par la suite tous les sommets et change leur étiquette selon la fonction d'évaluation que vous avez implémentée plus tôt. L'algorithme continue la propagation d'étiquette à travers tous les sommets jusqu'à un optimun de la modularité.\n","\n","#### Implémentation\n","1. Ajouter les étiquettes initiales aux sommets du graphe dans la fonction `__init__`. Il faut que chaque sommet soit dans sa propre communauté au début de l'algorithme. Initialiser le paramètre `labels` pour qu'il contient la liste des étiquettes présentes dans le réseau.\n","\n","2. Implémenter l'algorithme LPAm dans la fonction `LPAm`. Assurez-vous de toujours augmenter la modularité lors de vos changements d'étiquettes. N'oubliez pas de garder le paramètre `labels` à jour à fur et à mesure lors de vos changements pour ne pas évaluer plusieurs fois la même étiquette.\n","\n","Utilisez la fonction `test_lpam` pour vérifier votre implémentation. Vous devriez finir avec une modularité d'environ 0.399 avec 4 communautés.\n","\n","---\n","\n","You can now implement the LPAm algorithm. This algorithm is the predecessor of LPAm+ since LPAm+ was created to overcome LPAm's weakness. LPAm is a label probagation algorithm based on modularity. It begins by giving a unique label to each node. It then explores all the nodes and changes their label according to the evaluation function that you implemented earlier. The algorithm continues until it can no longer improve the modularity of the network.\n","\n","#### Implementation\n","1. Add the initial labels to the nodes in the graph in the function `__init__`. Each nodes has to be in their own community in the beginning. Initialise `labels` with the current list of labels present in the graph.\n","\n","2. Implement the LPAm algorithm in the function`LPAm`. Make sure that all your labels changes improve the modularity. Don't forget to keep your `labels` parameter is kept up-to-date so that you dont evaluate the same label multiple times or unused labels.\n","\n","Use the function `test_lpam` to verify your implementation. You should have a modularity of 0.399 with 4 communities."]},{"cell_type":"code","metadata":{"id":"O2RfrO6PSriL"},"source":["def test_lpam():\n","    social_network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n","    lpam = LPAmPlus(social_network)\n","    lpam.LPAm()\n","    print(\"Modularity: {:.3f}\\nCommunities: {}\".format(lpam.modularity(), lpam.labels))\n","\n","test_lpam()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cj5Ghd5jXGz6"},"source":["### 1.5 LPAm+ (3 point)\n","\n","Vous pouvez maintenant implémenter LPAm+ au complet. LPAm+ est une amélioration de LPAm. Lorsque LPAm tombe dans un optimum local, LPAm+ essaye de combiner deux communautés pour augmenter la modularité et ainsi sortir du optimum local. LPAm+ choisit la combinaison qui augmente le plus la modularité et recommence la propagation d'étiquette jusqu'au prochain optimum local où il va reessayer de combiner des communautés. L'algorithme continue jusqu'à qu'il ne peut plus augmenter la modularité.\n","\n","#### Implémentation\n","1. Implémentez  la fonction  `merge_communities`. Cette fonction regarde si combiner des communautés augmente la modularité et combine le meilleur choix. Elle retourne True si une combinaison a été faite sinon False (aucune combinaison augmente la modularité).\n","2. Implémenter `find_communities`. Cette fonction applique l'algorithme LPAm+ sur le réseau en utilisant les fonctions `LPAm` et `merge_communities`.\n","\n","Utilisez la fonction `test_lpam_plus` pour vérifier votre implémentation. Vous devriez finir avec une modularité d'environ 0.413 et 2 communautés.\n","\n","---\n","\n","You can now fully implement LPAm+. As said before LPAm+ is an amelioration of LPAm. The issue with LPAm is that it stops when it finds a local optimun. To prevent that, LPAm+ tries to combine two communities to increase modularity and escape the local optimun. LPAm+ chooses the combination that most increases modularity and restart the label's propagation until the next local optimum where it will try to combine two communities again. The algorithm continues until it can no longer increase modularity.\n","\n","#### Implementation\n","1. Implement the function  `merge_communities`. This function check if combining communities improve the modularity and combine the best choice. It returns True if a combinaison was made else False (no combination increase the modularity).\n","2. Implement the LPAM+ algorithm in the function `find_communities` using the fonctions `LPam` and `merge_communities`.\n","\n","Use the function `test_lpam_plus` to verify your implementation. You should end with a modularity of 0.413 and 2 communities."]},{"cell_type":"code","metadata":{"id":"eVSnUWQoSriP"},"source":["def test_lpam_plus():\n","    social_network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n","    lpam = LPAmPlus(social_network)\n","    lpam.find_communities()\n","    print(\"Modularity: {:.3f}\\nCommunities: {}\".format(lpam.modularity(), lpam.labels))\n","\n","test_lpam_plus()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vODCJbRaXGz-"},"source":["### 1.6 GOT dataset (3 points)\n","\n","Rouler votre algorithme sur les données de Games of Thrones de chaque saison et comparer ce que vous obtenez et les vraies communautés. Le ground truth se trouve dans la colonne Community des csv. Des liens sont présents entre des personnages lorsque: \n","- Personnage A parle directement après Personnage B\n","- Personnage A parle de Personnage B\n","- Personnage C parle de Personnage A et Personnage B\n","- Personnage A et Personnage B font une action ensemble dans une scène (ex: quittent les lieux, A regarde B, sont assis à une table, etc)\n","- Personnage A et Personnage B apparaissent ensemble dans une scène\n","\n","Commencez par calculer le RI (Rand index) de vos résultats. $$ RI=\\frac{TP+TN}{TP+TN+FP+FN} = \\frac{TP+TN}{\\binom{n}{2}}$$\n","\n","- n: le nombre de sommets\n","- TP: True positive soit le nombre de paires d'éléments qui se trouvent dans la même communauté dans vos résultats et dans le ground truth\n","- TN: True négative soit le nombre de paires d'éléments qui se trouvent dans des communautés différentes dans vos résultats et dans le ground truth\n","- FP: False positive soit le nombre de paires d'éléments qui se trouvent dans la même communauté dans vos résultats mais qui sont dans des communautés différentes dans le ground truth\n","- FN: False négative soit le nombre de paires d'éléments qui se trouvent dans des communautés différentes alors qu'ils sont dans la même communauté dans le ground truth\n","\n","\n","**N.B**: Ce n'est pas le nom des communautés que vous avez trouvé qui importante mais leur composition. Autrement dit, un TP est si le sommet a et le sommet b se trouve dans la même communauté dans vos résultats et dans le ground truth.\n","\n","\n","Répondez aux questions suivantes. Elles servent comme piste de réflexion pour votre analyse.\n","\n","- L'algorithme performe-t-il bien sur toutes les saisons ou pour certaines seulement? \n","- Expliquez pourquoi vous avez obtenu ces résultats en analysant la formation des communautés dans chaque saison. Quelles particularités favorisent des bons résultats? Quelles particularités nuisent à l'algorithme?\n","\n","Vous pouvez faire les manipulations que vous voulez pour mieux présenter vos résultats et mieux appuyer vos affirmations. \n","\n","---\n","\n","Run your algorithm over the Games of Thrones data from each season and compare what you get and the real communities. The ground truth is found in the Community column in the csv. Links are found between characters A and B when:\n","- Character A talks directly after Character B\n","- Character A talks about Character B\n","- Character C talks about Character B and A\n","- Character A and Character B does an action together in a scene (ex: leave the room, A looks toward B, are seated together at a table, etc)\n","- Character A and Character B are both present in a scene\n","\n","Start by calculating the RI (Rand index) of your results. $$ RI=\\frac{TP+TN}{TP+TN+FP+FN} = \\frac{TP+TN}{\\binom{n}{2}}$$\n","\n","- n: number of nodes\n","- TP: True positive the number of pairs of elements that are in the same community in your results and in the ground truth\n","- TN: True negative the number of pairs of elements that are in different communities in your results and in the ground truth\n","- FP: False positive the number of pairs of elements which are in the same community in your results but which are in different communities in the ground truth\n","- FN: False negative the number of pairs of elements which are in different communities in your results but which are in the same community in the ground truth\n","\n","**N.B:** What matters here is the composition of the communities you found not the names. A TP is when the node a and the node b are both in the same communities in your result and in the ground truth.\n","\n","Answer the following questions. They are guides for your analysis.\n","\n","- Does the algorithm perform well on all seasons or for some only? \n","- Explain why you obtained those results by analysing the communities from each season. Which particularities offer better results? Which hinder the algorithm?\n","\n","You can do the manipulations you want to better present your results and better support your statements."]},{"cell_type":"markdown","metadata":{"id":"ZzndRBWVSriT"},"source":["#### Résultats / Results"]},{"cell_type":"code","metadata":{"id":"1-2UipMRXG0R"},"source":["# Mettez votre code ici\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u-HMg4sUSriX"},"source":["#### Analyse / Analysis\n","Écrivez votre analyse ici / Write your analysis here"]},{"cell_type":"markdown","metadata":{"id":"r3FmllqgXG0d"},"source":["# 2. Personnages influents dans GOT / Influent character in GOT (6 points)\n","\n","##  Analyse d'un réseau social \n","Une autre analyse intéressante à faire avec un réseau social est de trouver les personnes influentes du réseau soit les personnes autour desquelles les gens du réseau se regroupent.\n","\n","Il existe des mesures qui permettent de connaître ces personnes: les mesures de centralité. **Vous devez implémenter les mesures vous-même et ne pas utilisez les implémentations de `networkx` de ces mesures.** Pour vous aider lors de l'implémentation de ses mesures, un deuxième toy dataset vous est fourni. Il ressemble à ceci (data/picture2.png):\n","![title](data/picture2.png)\n","\n","## GOT datasets\n","La série Games of Thrones est reconnue pour tuer ses personnages importants. Nous vous demandons de vérifier cette affirmation. Pour cette partie, vous devez utiliser tous les CSV donnés avec le TP (nodes, edges et deaths). Nous voulons que vous trouviez les personnages les plus influents de chaque saison et que vous les compariez avec la liste de personnages morts durant la saison.\n","\n","---\n","\n","##  Social network analysis\n","\n","Another interesting analysis to do with a social network is to find the influential people in the network, ie the people around whom the people in the network gather.\n","\n","There are measures which make it possible to know these people: the centrality measures. **You must implement those metrics yourselves. Do not use `networkx` implementation for the  tp.** To help you during the implementation of those measurements, a second toy dataset is provided to you. It looks like this (data/picture2.png): ![title](data/picture2.png)\n","\n","## GOT datasets\n","The Games of Thrones series is known to kill its important characters. We ask you to verify this statement. For this part, you must use all the csv given with the TP (nodes, edges and deaths). We want you to find the most influential characters from each season and compare them with the list of dead characters during the season.\n"]},{"cell_type":"markdown","metadata":{"id":"uJxSGCnOXG0e"},"source":["## 2.1 Centralité de degré / Degree centrality (1 point)\n","\n","Une première mesure simple pour trouver l'importance d'un sommet dans un réseau est la centralité de degré. Elle se calcule $$C_{D}(i) = \\frac{degree(i)}{n-1}$$\n","\n","- i: un sommet dans le réseau\n","- n: le nombre de sommets\n","- degree: le nombre d'arêtes attachées au sommet\n","\n","#### Implémentation\n","1. Implémenter la fonction `calculate_degree_centrality`. Cette fonction calcule la centralité de degré pour tous les sommets du réseau et ajoute cette mesure à chaque sommet.\n","\n","Utilisez la fonction `test_degree_centrality` pour vérifier votre implémentation. Le sommet 1 devrait avoir la plus haute mesure de 0.4.\n","\n","---\n","\n","A first simple measure to find the importance of a node in a network is the degree centrality. It is calculated $$C_{D}(i) = \\frac{degree(i)}{n-1}$$\n","\n","- i: a node in the network\n","- n: the number of nodes\n","- degree: the number of edges attached to the node\n","\n","#### Implementation\n","1. Implement the function `calculate_degree_centrality`. This function calculates degree centrality for all nodes in the network and adds this measurement to each node.\n","\n","Use the function `test_degree_centrality` to verify your implementation. The best node should be node 1 with 0.4."]},{"cell_type":"code","metadata":{"id":"vLTg0prlSriZ"},"source":["def calculate_degree_centrality(social_network):\n","    #TODO"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ad3lK2B8Srie"},"source":["def test_degree_centrality():\n","    social_network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n","    calculate_degree_centrality(social_network)\n","    dict_centrality = nx.get_node_attributes(social_network, 'degree_centrality')\n","    best_node = max(dict_centrality, key=dict_centrality.get)\n","    print(\"Highest degree centrality node: {} with {:.3f}\".format(best_node, dict_centrality[best_node]))\n","test_degree_centrality()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fhPWALjLSrig"},"source":["## 2.2 Centralité de proximité / Closeness centrality (1 point)\n","\n","Une autre mesure simple pour trouver l'importance d'un sommet dans un réseau est la centralité de proximité. Elle se calcule $$C_{P}(i) = \\frac{1}{AvDist(i)}$$\n","\n","- i: un sommet dans le réseau\n","- AvDist: la moyenne de toutes les distances les plus courtes pour atteindre chaque sommet à partir du sommet i\n","\n","#### Implémentation\n","1. Implémenter la fonction `calculate_closeness_centrality`. Cette fonction calcule la centralité de proximité pour tous les sommets du réseau et ajoute cette mesure à chaque sommet. Considérer chaque arête comme une distance de 1.\n","\n","**NB**: Utiliser la fonction `shortest_path()` du module Networkx pour trouver le chemin le plus court entre des sommets\n","\n","Utilisez la fonction `test_closeness_centrality` pour vérifier votre implémentation. Le sommet 7 devrait avoir la plus haute mesure de 0.577.\n","\n","---\n","\n","Another simple measure for finding the importance of a node in a network is closeness centrality. It is calculated $$C_{P}(i) = \\frac{1}{AvDist(i)}$$\n","\n","- i: a node in the network\n","- AvDist: the average of all shortest distances to reach each vertex from vertex i\n","\n","#### Implementation\n","1. Implement the function `calculate_closeness_centrality`. This function calculates closeness centrality for all nodes in the network and adds this measurement to each node. Consider each edge as a distance of 1.\n","\n","**NB**: Use the fucntion `shortest_path()` from Networkx to find the shortest path between two nodes.\n","\n","Use the function `test_closeness_centrality` to verify your implementation. The best node should be node 7 with 0.577."]},{"cell_type":"code","metadata":{"id":"534teZVYSrii"},"source":["def calculate_closeness_centrality(social_network):\n","    #TODO"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wVQG0XiFSril"},"source":["def test_closeness_centrality():\n","    social_network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n","    calculate_closeness_centrality(social_network)\n","    dict_centrality = nx.get_node_attributes(social_network, 'closeness_centrality')\n","    best_node = max(dict_centrality, key=dict_centrality.get)\n","    print(\"Highest closeness centrality node: {} with {:.3f}\".format(best_node, dict_centrality[best_node]))\n","\n","test_closeness_centrality()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OHTdy58OSrir"},"source":["## 2.3 Centralité d'intermédiarité / Betweeness centrality (1 point)\n","\n","Une dernière mesure simple pour trouver l'importance d'un sommet dans un réseau est la centralité d'intermédiarité. Elle se calcule $$C_{I}(i) = \\frac{\\sum_{j<k}f_{jk}(i)}{\\binom{n}{2}}$$\n","\n","- n: le nombre de sommets dans le réseau\n","- i: un sommet dans le réseau\n","- j,k: deux sommets dans le réseau excluant i\n","- $f_{jk}(i)$: le nombre de chemin le plus court partant du sommet j vers un sommet k (> j) passant par le sommet i \n","\n","#### Implémentation\n","1. Implémenter la fonction `calculate_betweenness_centrality`. Cette fonction calcule la centralité d'intermédiarité pour tous les sommets du réseau et ajoute cette mesure à chaque sommet.\n","\n","Utilisez la fonction `test_betweennes_centrality` pour vérifier votre implémentation. Le sommet 7 devrait avoir la plus haute mesure de 0.45.\n","\n","---\n","\n","A final simple measure to find the importance of a node in a network is the betweeness centrality. It is calculated $$C_{I}(i) = \\frac{\\sum_{j<k}f_{jk}(i)}{\\binom{n}{2}}$$\n","\n","- n: the number of nodes in the network\n","- i: a node in the network\n","- j,k: two nodes in the network excluding i\n","- $f_{jk}(i)$: the  number of shortest paths from vertex j to vertex k (> j) passing through node i\n","\n","#### Implementation\n","1. Implement the function `calculate_betweenness_centrality`.This function calculates the betweenness centrality for all the nodes of the network and adds this measurement to each node.\n","\n","Use the function `test_betweennes_centrality` to verify your implementation. The best node should be the node 7 with 0.45.\n"]},{"cell_type":"code","metadata":{"id":"_D2n5C0JSris"},"source":["def calculate_betweenness_centrality(social_network):\n","    #TODO"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PGVYdrAOSriu"},"source":["def test_betweenness_centrality():\n","    social_network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n","    calculate_betweenness_centrality(social_network)\n","    dict_centrality = nx.get_node_attributes(social_network, 'betweenness_centrality')\n","    best_node = max(dict_centrality, key=dict_centrality.get)\n","    print(\"Highest betweenness centrality node: {} with {:.3f}\".format(best_node, dict_centrality[best_node]))\n","\n","test_betweenness_centrality()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"996_M0-sXG0w"},"source":["## 2.4 Analyse de vos résultats / Analysis of your results (3 points)\n","\n","Executez les trois fonctions sur les réseaux de chaque saison et présentez le top 10 pour chaque mesure. **Les graphes des saisons 2, 4 et 6 sont déconnectés. Dans ce cas, considérez la plus grande composante connexe**. Pour chaque saison, comparez le top 10 des mesures avec la liste de morts de la saison disponible dans les csv death. Répondez aux questions suivantes. Elles sont des pistes de réflexions pour votre analyse.\n","\n","- Est-ce que le top 10 est suffisant pour trouver les morts importants de chaque saison? \n","- Quelle mesure semble mieux prédire les morts? \n","- Est-ce que la réputation de Games of Thrones de tuer plusieurs de ses personnages importants est fondée?\n","\n","**N.B.:** Si vous ne connaissez pas la série et vous n'êtes pas sûrs quels morts peuvent être considérés importants, faites une recherche Google sur les personnages importants. Mentionnez votre démarche et la conclusion de vos recherches. Il n'y a pas une liste précise de morts importants. Évidemment si vous me dite que Daenerys n'est pas importante, je vais douter de vos recherches. Le but est de voir votre travail de réflexion et d'analyse des mesures de centralité. \n","\n","---\n","\n","Run the three functions on the networks of each season and present the top 10 for each metric. **The season 2, 4 and 6 graphs are disconnected. In that case, consider the largest connected component**. For each season, compare the top 10 metrics with the season's death list in the death csv. Answer the following questions. They are guide for your analysis.\n","\n","- Is the top 10 enough to find the significant deaths of each season? \n","- What measure seems to better predict the dead? \n","- Is the reputation of Games of Thrones for killing many important characters founded?\n","\n","**N.B:** If you don't know the series and aren't sure which deaths are considered important, do a Google research on the important characters in the series. Metion your research and the conclusion of it. There isn't a precise list of important deaths but if you tell me that Daenerys isn't important, I will doubt of the seriousness of your research. The goal is to see how your analyse the results giving by centrality metrics."]},{"cell_type":"markdown","metadata":{"id":"y_S1YvI0Sriy"},"source":["### Résultats / Results"]},{"cell_type":"code","metadata":{"id":"mtYPFnJ3Sri2"},"source":["# Mettez le code pour présenter les résultats ici"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DeeyaJnKnDGB"},"source":["### Analyse / Analysis\n","\n","Écrivez ici"]}]}