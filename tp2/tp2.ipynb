{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mh2MiuVMazk"
   },
   "source": [
    "# TP2 - Market Basket Analysis \n",
    "INF8111 - Fouille de donn√©es, Summer 2020\n",
    "### Membres de l'√©quipe\n",
    "    - Membre 1\n",
    "    - Membre 2\n",
    "    - Membre 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmJEz5JEMazl"
   },
   "source": [
    "## Date et directives de remise\n",
    "Vous remettrez ce fichier nomm√© dans la bo√Æte de remise sur moodle. \n",
    "\n",
    "\n",
    "## Market Basket Analysis\n",
    "\n",
    "Le *Market Basket Analysis* (MBA) est une technique d'analyse de la fouille de donn√©es qui permet de d√©couvrir les associations entre les produits ou leur regroupement. En explorant des motifs int√©ressants √† partir d'une vaste collection de donn√©es, le MBA vise √† comprendre / r√©v√©ler les comportements d'achat des clients en se basant sur la th√©orie selon laquelle si vous avez achet√© un certain ensemble de produits, vous √™tes plus (ou moins) susceptible d'acheter un autre groupe de produits. En d'autres termes, le MBA permet aux d√©taillants d'identifier la relation entre les articles que les clients ach√®tent, r√©v√©lant des tendances d'articles souvent achet√©s ensemble.\n",
    "\n",
    "Une approche largement utilis√©e pour explorer ces motifs consiste √† construire *** des r√®gles d'association *** telles que\n",
    "- **si** achet√© *ITEM_1* **alors** ach√®tera *ITEM_2* avec **confiance** *X*.\n",
    "\n",
    "Ces associations n'ont pas √† √™tre des r√®gles individuelles. Ils peuvent impliquer de nombreux √©l√©ments. Par exemple, une personne dans un supermarch√© peut ajouter des ≈ìufs dans son panier, puis le MBA peut sugg√©rer qu'elle ach√®tera √©galement du pain et/ou de la farine:\n",
    "\n",
    "+ **si**  achet√© *OEUFS* **alors** ach√®tera [*PAIN* avec confiance *0,2*; *FARINE* avec confiance 0,05].\n",
    "\n",
    "Cependant, si la personne d√©cide maintenant d'ajouter de la farine √† son panier, la nouvelle r√®gle d'association pourrait √™tre comme ci-dessous, sugg√©rant des ingr√©dients pour faire un g√¢teau.\n",
    "\n",
    "+ **si** achet√© [*OEUFS, FARINE*] **alors** ach√®tera [*SUCRE* avec confiance 0,45; LEVURE avec confiance 0,12; *PAIN* avec confiance *0,03*].\n",
    "\n",
    "Il existe de nombreux sc√©narios r√©els o√π le MBA joue un r√¥le central dans l'analyse des donn√©es, comme les transactions de supermarch√©, les commandes en ligne ou l'historique des cartes de cr√©dit. Les sp√©cialistes du marketing peuvent utiliser ces r√®gles d'association pour organiser les produits corr√©l√©s plus pr√®s les uns des autres sur les √©tag√®res des magasins ou faire des suggestions en ligne afin que les clients ach√®tent plus d'articles. Un MBA peut g√©n√©ralement aider les d√©taillants √† r√©pondre aux questions les suivantes:\n",
    "\n",
    "- Quels articles sont souvent achet√©s ensemble ?\n",
    "- √âtant donn√© un panier, quels articles sugg√©rer ?\n",
    "- Comment placer les articles ensemble sur les √©tag√®res ?\n",
    "\n",
    "### Objectif\n",
    "\n",
    "Votre objectif dans ce TP est de d√©velopper un algorithme MBA pour r√©v√©ler les motifs en cr√©ant des r√®gles d'association dans un ensemble de donn√©es volumineux avec plus de trois millions de transactions de supermarch√©. Cependant, la collecte de r√®gles d'association dans les grands ensembles de donn√©es est un probl√®me tr√®s intensif en calcul, ce qui rend presque impossible leur ex√©cution sans syst√®me distribu√©. Par cons√©quent, pour ex√©cuter votre algorithme, vous aurez acc√®s √† un cluster de *cloud computing* distribu√© avec des centaines de c≈ìurs.\n",
    "\n",
    "√Ä cette fin, un algorithme **MapReduce** sera impl√©ment√© avec le framework [Apache Spark](http://spark.apache.org), un syst√®me informatique distribu√© rapide. En r√©sum√©, Spark est un framework open source con√ßu avec une m√©thodologie *scale-out* qui en fait un outil tr√®s puissant pour les programmeurs ou les d√©veloppeurs d'applications pour effectuer un volume massif de calculs et de traitement de donn√©es dans des environnements distribu√©s. Spark fournit des API de haut niveau qui facilitent la cr√©ation d'applications parall√®les sans avoir √† se soucier de la fa√ßon dont votre code et vos donn√©es sont parall√©lis√©s / distribu√©s par le cluster informatique. Spark fait tout pour vous.\n",
    "\n",
    "La mise en ≈ìuvre suivra l'algorithme d'analyse du panier de march√© pr√©sent√© par Jongwook Woo et Yuhang Xu (2012). L'image **workflow.pdf** illustre le flux de travail de l'algorithme et doit √™tre utilis√©e pour consultation tout au long de ce TP. Les cases bleues sont celles o√π vous devez impl√©menter une m√©thode pour effectuer une fonction de mappage ou de r√©duction, et les cases grises repr√©sentent leur sortie attendue. **Toutes ces op√©rations sont expliqu√©es en d√©tail dans les sections suivantes.**\n",
    "\n",
    "## 1. Configuration de Spark\n",
    "\n",
    "Spark fonctionne sur les syst√®mes Windows et UNIX (par exemple, Linux, Mac OS). Il est facile d'ex√©cuter Spark localement sur une seule machine - tout ce dont vous avez besoin est d'avoir Java install√© sur votre syst√®me PATH, ou la variable d'environnement JAVA_HOME pointant vers une installation Java. Il est obligatoire que le **JDK v8/11** soit install√© sur votre syst√®me, car Spark ne prend actuellement en charge que cette version. Si ce n'est pas le cas, acc√©dez √† [la page Web de Java](https://www.oracle.com/java/technologies/javase-downloads.html) pour t√©l√©charger et installer une machine virtuelle Java. N'oubliez pas de d√©finir la variable d'environnement JAVA_HOME pour utiliser JDK v8/11 si votre installation ne le fait pas automatiquement.\n",
    "\n",
    "L'interface entre Python et Spark se fait via **PySpark**, qui peut √™tre install√© en ex√©cutant `pip install pyspark` ou configur√© en suivant la s√©quence ci-dessous:\n",
    "\n",
    "1. D'abord, allez sur http://spark.apache.org/downloads\n",
    "2. S√©lectionnez la derni√®re version de Spark et le package pr√©-construit pour Apache Hadoop 2.7\n",
    "3. Cliquez pour t√©l√©charger **spark-3.1.1-bin-hadoop2.7.tgz** et d√©compressez-le dans le dossier de votre choix.\n",
    "4. Ensuite, exportez les variables suivantes pour lier PYSPARK (l'interface python de Spark) √† votre distribution python dans votre fichier `~/.bash_profile`.\n",
    "\n",
    "``\n",
    "export SPARK_HOME=/chemin/vers/spark-3.1.1-bin-hadoop2.7\n",
    "export PYTHONPATH=\"$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$SPARK_HOME/python/lib/pyspark.zip:$PYTHONPATH\"\n",
    "export PYSPARK_PYTHON=/chemin/vers/votre/python3\n",
    "``\n",
    "\n",
    "5. Ex√©cutez `source ~./bash_profile` pour effectuer les modifications et red√©marrer cette session de notebook jupyter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmUMt4htMazm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rovSCW_vYs7m"
   },
   "source": [
    "#### Testez votre Spark\n",
    "√Ä l'aide du code suivant, vous pouvez tester si Spark est install√© correctement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxgNiBFkYs7n"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.sql(\"select 'spark' as hello \")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaYpUpOFMazu"
   },
   "source": [
    "### 1.1 Exemple de comptage de produits \n",
    "\n",
    "Pour tester votre installation et commencer √† vous familiariser avec Spark, nous suivrons un exemple qui compte combien de fois les produits d'un toy dataset ont √©t√© achet√©s.\n",
    "\n",
    "Le principal point d'entr√©e pour commencer la programmation avec Spark est [l'API RDD](https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.html#rdd-apis), une excellente abstraction Spark pour travailler avec MapReduce. RDD est une collection d'√©l√©ments partitionn√©s sur les n≈ìuds du cluster qui peuvent fonctionner en parall√®le. En d'autres termes, RDD est la fa√ßon dont Spark maintient vos donn√©es pr√™tes √† ex√©cuter une fonction (par exemple, une fonction Map ou une fonction reduce) en parall√®le. **Ne vous inqui√©tez pas si cela semble toujours d√©routant, il sera clair une fois que vous commencerez √† l'impl√©menter**. Cependant, cela fait partie de ce TP d'√©tudier / consulter [Spark python API](https://spark.apache.org/docs/latest/api/python/) et d'apprendre √† l'utiliser. Certaines fonctions utiles offertes par l'API RDD sont:\n",
    "\n",
    "1. **map**: return a new RDD by applying a function to each element of this RDD.\n",
    "2. **flatMap**: return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. **Should be used when each entry will yield more than one mapped element**\n",
    "3. **reduce**: reduces the elements of this RDD using the specified commutative and associative binary operator.\n",
    "4. **reduceByKey**: merge the values for each key using an associative and commutative reduce function\n",
    "5. **groupByKey**: group the values for each key in the RDD into a single sequence\n",
    "6. **collect**: return a list that contains all of the elements in this RDD. **Should not be used when working with a lot of data**\n",
    "7. **sample**: return a sampled subset of this RDD\n",
    "8. **count**: return the number of elements in this RDD.\n",
    "9. **filter**: return a new RDD containing only the elements that satisfy a predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZDz1nrBMazu"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def map_to_product(row):\n",
    "    \"\"\"\n",
    "    Map each transaction into a set of KEY-VALUE elements.\n",
    "    The KEY is the word (product) itself and the VALUE is its number of apparitions.\n",
    "    \"\"\"\n",
    "    products = row.transaction.split(';') # split products from the column transaction\n",
    "    for p in products:\n",
    "        yield (p, 1)\n",
    "\n",
    "def reduce_product_by_key(value1, value2):\n",
    "    \"Reduce the mapped objects to unique words by merging (summing ) their values\"\n",
    "    return value1+value2\n",
    "\n",
    "# Initializates a object of SparkSession class, main entry point to Spark's funcionalites\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "        \n",
    "# Read a toy dataset\n",
    "toy = spark.read.csv('toy.csv', header=True)\n",
    "print(\"Toy dataset\")\n",
    "toy.show()\n",
    "\n",
    "# Obtain a RDD object to call a map function\n",
    "toy_rdd = toy.rdd\n",
    "print(\"Toy dataframe as a RDD object (list of Row objects):\\n\\t\", toy_rdd.collect())\n",
    "\n",
    "# Map function to identify all products\n",
    "toy_rdd = toy_rdd.flatMap(map_to_product)\n",
    "print(\"\\nMapped products:\\n\\t\", toy_rdd.collect())\n",
    "\n",
    "# Reduce function to merge values of elements that share the same KEY\n",
    "toy_rdd = toy_rdd.reduceByKey(reduce_product_by_key)\n",
    "print(\"\\nReduced (merged) products:\\n\\t\", toy_rdd.collect())\n",
    "\n",
    "print(\"\\nVisualizing as a dataframe:\")\n",
    "toy_rdd.toDF([\"product\", \"count_product\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpJGQmzXMazz"
   },
   "source": [
    "### 1.2 Travailler avec Spark Dataframe\n",
    "\n",
    "Dans l'exemple ci-dessus, nous avons bri√®vement utilis√© une classe Dataframe de Spark, mais uniquement pour obtenir un objet RDD avec ``toy.rdd`` et pour aficher les donn√©es sous forme de tableau structur√© avec le ``show ()`` une fonction. Cependant, [Dataframe](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#) est une partie cruciale de la version actuelle de Spark et est construit sur l'API RDD. Il s'agit d'une collection distribu√©e de lignes sous des colonnes nomm√©es, identique √† une table dans une base de donn√©es relationnelle. Le Dataframe de Spark fonctionne de la m√™me mani√®re que [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html). En fait, nous pouvons exporter (obtenir) une Dataframe Spark vers (√† partir de) ‚Äã‚Äãune Dataframe pandas avec la fonction ``toPandas()``  (``spark.createDataFrame``).\n",
    "\n",
    "Une fonctionnalit√© centrale du Dataframe est de b√©n√©ficier du [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html#sql), un module qui permet des requ√™tes SQL sur des donn√©es structur√©es. Par exemple, le m√™me ¬´ exemple de comptage de produits ¬ª aurait pu √™tre impl√©ment√© comme une s√©quence d'op√©rations SQL sur les donn√©es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFL6BuIDMaz0"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "# Creates a new column, products, with all products appering in each transaction\n",
    "print('New column \\'products\\': exploding the transaction\\'s products to a new row')\n",
    "df_toy = toy.withColumn('products', f.explode(f.split(toy.transaction, ';')))\n",
    "df_toy.show()\n",
    "\n",
    "# Performs a select query and group rows by the product name, aggreagating by counting\n",
    "print('Couting unique products:')\n",
    "df_toy.select(df_toy.products)\\\n",
    "      .groupBy(df_toy.products)\\\n",
    "      .agg(f.count('products').alias('count_product'))\\\n",
    "      .sort('count_product', ascending=False)\\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4HFs8CVMaz3"
   },
   "source": [
    "En outre, les m√™mes op√©rations SQL effectu√©es ci-dessus auraient pu √™tre effectu√©es avec une requ√™te en langage SQL traditionnel comme indiqu√© ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_eYl-7tMaz3"
   },
   "outputs": [],
   "source": [
    "# Creates a relational table TOY in the Spark session\n",
    "df_toy.createOrReplaceTempView(\"TOY\")\n",
    "\n",
    "spark.sql(\"SELECT t.products, COUNT(t.products) AS product_count\"\n",
    "          \" FROM TOY t\"\n",
    "          \" GROUP BY t.products\"\n",
    "          \" ORDER BY product_count DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0Onr2NxMaz8"
   },
   "source": [
    "Ces concepts SQL sont mentionn√©s ici car ils nous seront utiles lors du TP, principalement dans la section 3, pour manipuler les donn√©es du supermarch√©, qui sont structur√©es en tableaux. Ainsi, si vous n'√™tes pas familier avec SQL, il est recommand√© de suivre un [tutoriel](https://www.w3schools.com/sql/) pour comprendre les bases.\n",
    "\n",
    "## 2. Algorithme MBA\n",
    "Les sections suivantes expliquent comment d√©velopper chaque √©tape de l'algorithme MapReduce pour notre application de supermarch√©. La figure workflow.pdf illustre chaque √©tape de l'algorithme.\n",
    "\n",
    "### 2.1 Map to Patterns (10 points)\n",
    "Pour un sous-ensemble de transactions (c'est-√†-dire les lignes de notre toy dataset), chaque transaction doit √™tre **mapp√©e** vers un ensemble de *motifs d'achat* trouv√©s dans la transaction. Formellement, ces motifs sont des sous-ensembles de produits qui repr√©sentent un groupe d'articles achet√©s ensemble. \n",
    "\n",
    "Pour le framework MapReduce, chaque motif doit √™tre cr√©√© comme un √©l√©ment *KEY-VALUE*, o√π la KEY peut prendre la forme d'un singleton, d'une paire ou d'un trio de produits pr√©sents dans la transaction. Plus pr√©cis√©ment, pour chaque transaction, la fonction de mappage doit g√©n√©rer tous les sous-ensembles **UNIQUE** possibles de taille **UN, DEUX ou TROIS**. La VALEUR associ√©e √† chaque KEY est le nombre de fois que la KEY est apparue dans la transaction (si nous supposons qu'aucun produit n'appara√Æt plus d'une fois dans la transaction, cette valeur est toujours √©gale √† un).\n",
    "\n",
    "Maintenant, impl√©mentez la fonction **map_to_patterns** qui re√ßoit une transaction (une ligne du dataset) et retourne les motifs trouv√©s dans la transaction. Les √©l√©ments mapp√©s sont un tuple (KEY, VALUE), o√π KEY est √©galement un tuple de noms de produits. Il est crucial de noter que, puisque chaque entr√©e (transaction) de la fonction MAP produira **plus** un √©l√©ment KEY-VALUE, un *flatMap* doit √™tre invoqu√© pour cette √©tape.\n",
    "\n",
    "Pour le toy dataset, la sortie attendue est similaire √†:\n",
    "\n",
    "\n",
    "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 20em; padding-left:1px\">\n",
    "<code>\n",
    "+---------------+-----------+\n",
    "|       patterns|occurrences|\n",
    "+---------------+-----------+\n",
    "|         ('a',)|          1|\n",
    "|     ('a', 'b')|          1|\n",
    "|('a', 'b', 'c')|          1|\n",
    "|('a', 'b', 'f')|          1|\n",
    "|     ('a', 'c')|          1|\n",
    "|('a', 'c', 'f')|          1|\n",
    "|     ('a', 'f')|          1|\n",
    "|         ('b',)|          1|\n",
    "|     ('b', 'c')|          1|\n",
    "|('b', 'c', 'f')|          1|\n",
    "|     ('b', 'f')|          1|\n",
    "|         ('c',)|          1|\n",
    "|     ('c', 'f')|          1|\n",
    "|         ('f',)|          1|\n",
    "|         ('a',)|          1|\n",
    "|     ('a', 'b')|          1|\n",
    "|('a', 'b', 'd')|          1|\n",
    "|('a', 'b', 'e')|          1|\n",
    "|     ('a', 'd')|          1|\n",
    "|('a', 'd', 'e')|          1|\n",
    "|     ('a', 'e')|          1|\n",
    "|         ('b',)|          1|\n",
    "|     ('b', 'd')|          1|\n",
    "|('b', 'd', 'e')|          1|\n",
    "|     ('b', 'e')|          1|\n",
    "|         ('d',)|          1|\n",
    "|     ('d', 'e')|          1|\n",
    "|         ('e',)|          1|\n",
    "|         ('b',)|          1|\n",
    "|     ('b', 'c')|          1|\n",
    "|         ('c',)|          1|\n",
    "|         ('b',)|          1|\n",
    "|     ('b', 'c')|          1|\n",
    "|         ('c',)|          1|\n",
    "+---------------+-----------+\n",
    "</code>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BscKKDAjMaz9"
   },
   "outputs": [],
   "source": [
    "def format_tuples(pattern):\n",
    "    \"\"\"\n",
    "    Used for visualizition.\n",
    "    Transforms tuples to a string since Dataframe does not support column of tuples with different sizes\n",
    "    (a,b,c) -> '(a,b,c)'\n",
    "    \"\"\"\n",
    "    return (str(pattern[0]), str(pattern[1]))\n",
    "\n",
    "def map_to_patterns(row):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "toy_rdd = toy.rdd\n",
    "patterns_rdd = toy_rdd.flatMap(map_to_patterns)\n",
    "\n",
    "# Output as dataframe\n",
    "patterns_rdd.map(format_tuples).toDF(['patterns', 'occurrences']).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvvRw0plMa0B"
   },
   "source": [
    "### 2.2  Reduce patterns  (2,5 points)\n",
    "Une fois que diff√©rents processeurs ont trait√© les transactions, une fonction **reduce** doit √™tre appel√©e pour combiner des KEYS identiques (le sous-ensemble de produits) et calculer le nombre total de ses occurrences dans le dataset. En d'autres termes, cette proc√©dure de r√©duction doit additionner la *VALUE* de chaque KEY identique.\n",
    "\n",
    "Cr√©ez ci-dessous une fonction **reduce_patterns** qui doit additionner la VALUE de chaque motif.\n",
    "Pour le toy dataset, la sortie attendue est:\n",
    "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 28em; padding-left:5px\">\n",
    "<code>\n",
    "+---------------+--------------------+\n",
    "|       patterns|combined_occurrences|\n",
    "+---------------+--------------------+\n",
    "|         ('a',)|                   2|\n",
    "|     ('a', 'b')|                   2|\n",
    "|('a', 'b', 'c')|                   1|\n",
    "|('a', 'b', 'f')|                   1|\n",
    "|     ('a', 'c')|                   1|\n",
    "|('a', 'c', 'f')|                   1|\n",
    "|     ('a', 'f')|                   1|\n",
    "|         ('b',)|                   4|\n",
    "|     ('b', 'c')|                   3|\n",
    "|('b', 'c', 'f')|                   1|\n",
    "|     ('b', 'f')|                   1|\n",
    "|         ('c',)|                   3|\n",
    "|     ('c', 'f')|                   1|\n",
    "|         ('f',)|                   1|\n",
    "|('a', 'b', 'd')|                   1|\n",
    "|('a', 'b', 'e')|                   1|\n",
    "|     ('a', 'd')|                   1|\n",
    "|('a', 'd', 'e')|                   1|\n",
    "|     ('a', 'e')|                   1|\n",
    "|     ('b', 'd')|                   1|\n",
    "|('b', 'd', 'e')|                   1|\n",
    "|     ('b', 'e')|                   1|\n",
    "|         ('d',)|                   1|\n",
    "|     ('d', 'e')|                   1|\n",
    "|         ('e',)|                   1|\n",
    "+---------------+--------------------+\n",
    "</code>\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67IKY_4MMa0C"
   },
   "outputs": [],
   "source": [
    "def reduce_patterns(...)\n",
    "\"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "combined_patterns_rdd = patterns_rdd.#TODO\n",
    "\n",
    "# Output as dataframe\n",
    "combined_patterns_rdd.map(format_tuples).toDF(['patterns', 'combined_occurrences']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BME1VugMa0F"
   },
   "source": [
    "### 2.3 Map to subpatterns (15 points)\n",
    "Ensuite, une autre fonction **map** doit √™tre appliqu√©e pour g√©n√©rer des sous-motifs. Encore une fois, les sous-motifs sont des √©l√©ments KEY-VALUE, o√π la KEY est √©galement un sous-ensemble de produits. Cependant, la cr√©ation de la KEY du sous-motif est une proc√©dure diff√©rente. Cette fois, l'id√©e est de d√©composer la liste des produits de chaque motif (KEY), de supprimer un produit √† la fois et de produire la liste r√©sultante en tant que nouvelle cl√© de sous-motif.\n",
    "\n",
    "Par exemple, pour un mod√®le donn√© $P$ avec trois produits, $p_1, p_2$ et $p_3$, trois nouvelles cl√©s de sous-motifs vont √™tre cr√©√©es: (i) supprimer $p_1$ et retourner ($p_2, p_3$) ; (ii) supprimer $p_2$ et retourner ($p_1, p_3$); et (iii) supprimer $p_3$ et retourner ($p_1, p_2$).\n",
    "\n",
    "De plus, la structure VALUE du sous-motif sera √©galement diff√©rente. Au lieu d'une seule valeur enti√®re unique comme nous l'avons eu dans les motifs, cette fois un *tuple* devrait √™tre cr√©√© pour le sous-motif VALUE. Ce tuple contient le produit qui a √©t√© retir√© lors de la remise de la KEY et le nombre de fois que le motif est apparu. Par exemple ci-dessus, les valeurs doivent √™tre ($p_1,v$), ($p_2,v$) et ($p_3,v $), respectivement, o√π $v$ est la VALEUR du motif.\n",
    "\n",
    "L'id√©e derri√®re les sous-motif est de cr√©er **des r√®gles** telles que : lorsque les produits de KEY ont √©t√© achet√©s, l'article pr√©sent dans la VALEUR a √©galement √©t√© achet√© *v* fois. En outre, chaque motif doit √©galement produire un sous-motif dans lequel la cl√© est la m√™me liste de produits du motif, mais la valeur est un tuple avec un produit nul (None) et le nombre de fois que le motif est apparu. Cet √©l√©ment sera utile pour garder une trace du nombre de fois o√π un tel motif a √©t√© trouv√© et sera utilis√© ult√©rieurement pour calculer la valeur de confiance lors de la g√©n√©ration des r√®gles d'association.\n",
    "\n",
    "**Impl√©mentez la fonction map_to_subpatterns qui re√ßoit un motif et produit tous les sous-motif trouv√©s. Encore une fois, chaque entr√©e (motif) g√©n√©rera plus d'un √©l√©ment KEY-VALUE, puis une fonction flatMap doit √™tre appel√©e.**\n",
    "\n",
    "Pour le toy dataset, la sortie attendue est:\n",
    "\n",
    "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 20em; padding-left:5px\">\n",
    "<code>\n",
    "+---------------+---------+\n",
    "|    subpatterns|    rules|\n",
    "+---------------+---------+\n",
    "|         ('a',)|(None, 2)|\n",
    "|     ('a', 'b')|(None, 2)|\n",
    "|         ('b',)| ('a', 2)|\n",
    "|         ('a',)| ('b', 2)|\n",
    "|('a', 'b', 'c')|(None, 1)|\n",
    "|     ('b', 'c')| ('a', 1)|\n",
    "|     ('a', 'c')| ('b', 1)|\n",
    "|     ('a', 'b')| ('c', 1)|\n",
    "|('a', 'b', 'f')|(None, 1)|\n",
    "|     ('b', 'f')| ('a', 1)|\n",
    "|     ('a', 'f')| ('b', 1)|\n",
    "|     ('a', 'b')| ('f', 1)|\n",
    "|     ('a', 'c')|(None, 1)|\n",
    "|         ('c',)| ('a', 1)|\n",
    "|         ('a',)| ('c', 1)|\n",
    "|('a', 'c', 'f')|(None, 1)|\n",
    "|     ('c', 'f')| ('a', 1)|\n",
    "|     ('a', 'f')| ('c', 1)|\n",
    "|     ('a', 'c')| ('f', 1)|\n",
    "|     ('a', 'f')|(None, 1)|\n",
    "|         ('f',)| ('a', 1)|\n",
    "|         ('a',)| ('f', 1)|\n",
    "|         ('b',)|(None, 4)|\n",
    "|     ('b', 'c')|(None, 3)|\n",
    "|         ('c',)| ('b', 3)|\n",
    "|         ('b',)| ('c', 3)|\n",
    "|('b', 'c', 'f')|(None, 1)|\n",
    "|     ('c', 'f')| ('b', 1)|\n",
    "|     ('b', 'f')| ('c', 1)|\n",
    "|     ('b', 'c')| ('f', 1)|\n",
    "|     ('b', 'f')|(None, 1)|\n",
    "|         ('f',)| ('b', 1)|\n",
    "|         ('b',)| ('f', 1)|\n",
    "|         ('c',)|(None, 3)|\n",
    "|     ('c', 'f')|(None, 1)|\n",
    "|         ('f',)| ('c', 1)|\n",
    "|         ('c',)| ('f', 1)|\n",
    "|         ('f',)|(None, 1)|\n",
    "|('a', 'b', 'd')|(None, 1)|\n",
    "|     ('b', 'd')| ('a', 1)|\n",
    "|     ('a', 'd')| ('b', 1)|\n",
    "|     ('a', 'b')| ('d', 1)|\n",
    "|('a', 'b', 'e')|(None, 1)|\n",
    "|     ('b', 'e')| ('a', 1)|\n",
    "|     ('a', 'e')| ('b', 1)|\n",
    "|     ('a', 'b')| ('e', 1)|\n",
    "|     ('a', 'd')|(None, 1)|\n",
    "|         ('d',)| ('a', 1)|\n",
    "|         ('a',)| ('d', 1)|\n",
    "|('a', 'd', 'e')|(None, 1)|\n",
    "|     ('d', 'e')| ('a', 1)|\n",
    "|     ('a', 'e')| ('d', 1)|\n",
    "|     ('a', 'd')| ('e', 1)|\n",
    "|     ('a', 'e')|(None, 1)|\n",
    "|         ('e',)| ('a', 1)|\n",
    "|         ('a',)| ('e', 1)|\n",
    "|     ('b', 'd')|(None, 1)|\n",
    "|         ('d',)| ('b', 1)|\n",
    "|         ('b',)| ('d', 1)|\n",
    "|('b', 'd', 'e')|(None, 1)|\n",
    "|     ('d', 'e')| ('b', 1)|\n",
    "|     ('b', 'e')| ('d', 1)|\n",
    "|     ('b', 'd')| ('e', 1)|\n",
    "|     ('b', 'e')|(None, 1)|\n",
    "|         ('e',)| ('b', 1)|\n",
    "|         ('b',)| ('e', 1)|\n",
    "|         ('d',)|(None, 1)|\n",
    "|     ('d', 'e')|(None, 1)|\n",
    "|         ('e',)| ('d', 1)|\n",
    "|         ('d',)| ('e', 1)|\n",
    "|         ('e',)|(None, 1)|\n",
    "+---------------+---------+\n",
    "</code>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8aLrdMuMa0G"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def map_to_subpatterns(pattern):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "subpatterns_rdd = combined_patterns_rdd.#TODO\n",
    "\n",
    "# Output as dataframe\n",
    "subpatterns_rdd.map(format_tuples).toDF(['subpatterns', 'rules']).show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jl6TWh8rMa0J"
   },
   "source": [
    "### 2.4 Reduce Subpatterns (2.5 points)\n",
    "\n",
    "Encore une fois, une fonction **reduce** est n√©cessaire pour regrouper tous les sous-motif par leur KEY. L'objectif de cette proc√©dure de r√©duction est de cr√©er une liste de toutes les **r√®gles** apparues dans KEY. Par cons√©quent, la sortie attendue r√©sultant de cette fonction de r√©duction est √©galement un √©l√©ment KEY-VALUE, o√π la cl√© est la KEY du sous-motif et la valeur est un groupe contenant toutes les valeurs des sous-motif qui partagent la m√™me cl√©.\n",
    "\n",
    "Pour le toy dataset, la sortie attendue est:\n",
    "\n",
    "\n",
    "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 50em; padding-left:5px\">\n",
    "<code>\n",
    "+---------------+-------------------------------------------------------------+\n",
    "|subpatterns    |combined_rules                                               |\n",
    "+---------------+-------------------------------------------------------------+\n",
    "|('a',)         |[(None, 2), ('b', 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]|\n",
    "|('a', 'b')     |[(None, 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]          |\n",
    "|('b',)         |[('a', 2), (None, 4), ('c', 3), ('f', 1), ('d', 1), ('e', 1)]|\n",
    "|('a', 'b', 'c')|[(None, 1)]                                                  |\n",
    "|('b', 'c')     |[('a', 1), (None, 3), ('f', 1)]                              |\n",
    "|('a', 'c')     |[('b', 1), (None, 1), ('f', 1)]                              |\n",
    "|('a', 'b', 'f')|[(None, 1)]                                                  |\n",
    "|('b', 'f')     |[('a', 1), ('c', 1), (None, 1)]                              |\n",
    "|('a', 'f')     |[('b', 1), ('c', 1), (None, 1)]                              |\n",
    "|('c',)         |[('a', 1), ('b', 3), (None, 3), ('f', 1)]                    |\n",
    "|('a', 'c', 'f')|[(None, 1)]                                                  |\n",
    "|('c', 'f')     |[('a', 1), ('b', 1), (None, 1)]                              |\n",
    "|('f',)         |[('a', 1), ('b', 1), ('c', 1), (None, 1)]                    |\n",
    "|('b', 'c', 'f')|[(None, 1)]                                                  |\n",
    "|('a', 'b', 'd')|[(None, 1)]                                                  |\n",
    "|('b', 'd')     |[('a', 1), (None, 1), ('e', 1)]                              |\n",
    "|('a', 'd')     |[('b', 1), (None, 1), ('e', 1)]                              |\n",
    "|('a', 'b', 'e')|[(None, 1)]                                                  |\n",
    "|('b', 'e')     |[('a', 1), ('d', 1), (None, 1)]                              |\n",
    "|('a', 'e')     |[('b', 1), ('d', 1), (None, 1)]                              |\n",
    "+---------------+-------------------------------------------------------------+\n",
    "</code>\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOP-SVIhMa0J"
   },
   "outputs": [],
   "source": [
    "combined_rules = subpatterns_rdd.#TODO\n",
    "\n",
    "# Output as dataframe\n",
    "combined_rules.map(format_tuples).toDF(['subpatterns', 'combined_rules']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uh69x3a8Ma0P"
   },
   "source": [
    "### 2.5. Map to Association Rules (15 points)\n",
    "\n",
    "Enfin, la derni√®re √©tape de l'algorithme consiste √† cr√©er les r√®gles d'association pour effectuer la MBA. Le but de cette fonction Map est de calculer le niveau **de confiance** de l'achat d'un produit, sachant qu'il y a d√©j√† un ensemble de produits dans le panier. Ainsi, la KEY du sous-motif est l'ensemble des produits plac√©s dans le panier et, pour chaque produit pr√©sent dans la liste des r√®gles, c'est-√†-dire dans la VALEUR, la confiance peut √™tre calcul√©e comme :\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\text{nombre de fois o√π le produit a √©t√© achet√© avec KEY}}{\\text{nombre de fois o√π la KEY est apparue}}\n",
    "\\end{align*}\n",
    "\n",
    "Pour l'exemple donn√© dans la figure \"workflow\", *le caf√©* a √©t√© achet√© 20 fois et, dans 17 d'entre eux, le *lait* a √©t√© achet√© ensemble. Ensuite, le niveau de confiance pour acheter du *lait* sachant que *le caf√©* est dans le panier est $\\frac{17}{20}=0,85$, ce qui signifie que dans 85% des cas o√π le caf√© a √©t√© achet√©, le lait a aussi √©t√© achet√©.\n",
    "\n",
    "Impl√©mentez la fonction **map_to_assoc_rules** qui calcule le niveau de confiance pour chaque sous-motif.\n",
    "\n",
    "Pour le toy dataset, la sortie attendue est:\n",
    "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 57em; padding-left:5px\">\n",
    "<code>\n",
    "+---------------+------------------------------------------------------------------+\n",
    "|patterns       |association_rules                                                 |\n",
    "+---------------+------------------------------------------------------------------+\n",
    "|('a',)         |[('b', 1.0), ('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]      |\n",
    "|('a', 'b')     |[('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]                  |\n",
    "|('b',)         |[('a', 0.5), ('c', 0.75), ('f', 0.25), ('d', 0.25), ('e', 0.25)]  |\n",
    "|('a', 'b', 'c')|[]                                                                |\n",
    "|('b', 'c')     |[('a', 0.3333333333333333), ('f', 0.3333333333333333)]            |\n",
    "|('a', 'c')     |[('b', 1.0), ('f', 1.0)]                                          |\n",
    "|('a', 'b', 'f')|[]                                                                |\n",
    "|('b', 'f')     |[('a', 1.0), ('c', 1.0)]                                          |\n",
    "|('a', 'f')     |[('b', 1.0), ('c', 1.0)]                                          |\n",
    "|('c',)         |[('a', 0.3333333333333333), ('b', 1.0), ('f', 0.3333333333333333)]|\n",
    "|('a', 'c', 'f')|[]                                                                |\n",
    "|('c', 'f')     |[('a', 1.0), ('b', 1.0)]                                          |\n",
    "|('f',)         |[('a', 1.0), ('b', 1.0), ('c', 1.0)]                              |\n",
    "|('b', 'c', 'f')|[]                                                                |\n",
    "|('a', 'b', 'd')|[]                                                                |\n",
    "|('b', 'd')     |[('a', 1.0), ('e', 1.0)]                                          |\n",
    "|('a', 'd')     |[('b', 1.0), ('e', 1.0)]                                          |\n",
    "|('a', 'b', 'e')|[]                                                                |\n",
    "|('b', 'e')     |[('a', 1.0), ('d', 1.0)]                                          |\n",
    "|('a', 'e')     |[('b', 1.0), ('d', 1.0)]                                          |\n",
    "+---------------+------------------------------------------------------------------+\n",
    "</code>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPrbn5CfMa0P"
   },
   "outputs": [],
   "source": [
    "def map_to_assoc_rules(rule):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "\n",
    "assoc_rules = combined_rules.#TODO\n",
    "\n",
    "# Output as dataframe\n",
    "assoc_rules.map(format_tuples).toDF(['patterns', 'association_rules']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPV5g2hwMa0U"
   },
   "source": [
    "## 3. Instacart dataset\n",
    "\n",
    "Avec votre algorithme MBA pr√™t √† √™tre utilis√©, il est maintenant temps de travailler sur l'ensemble de donn√©es r√©el. Pour cette partie du TP, t√©l√©chargez le dataset [instacart](https://www.dropbox.com/s/qa7nsw2at3hsbmp/instacart.zip?dl=0) et lisez sa [description](https://gist.github.com/jeremystan/c3b39d947d9b88b3ccff3147dbcf6c6b) pour comprendre la structure de l'ensemble de donn√©es.\n",
    "\n",
    "Avant d'appliquer l'algorithme d√©velopp√© sur l'ensemble de donn√©es instacart, vous devez d'abord filtrer les transactions pour qu'elles soient au m√™me format d√©fini par votre algorithme (une transaction par ligne). Pour manipuler les donn√©es, nous pouvons utiliser le bloc de donn√©es de Spark et le module SQL pr√©sent√© dans la section 1.\n",
    "\n",
    "La cellule de code suivante utilise le module Spark SQL pour lire les commandes de ``order_products__train.csv`` et les informations d√©taill√©es de ``orders.csv`` et ``products.csv`` pour construire une dataframe qui contient un liste de tous les produits jamais achet√©s par chaque utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oB1eTkeMa0W"
   },
   "outputs": [],
   "source": [
    "df_order_prod = spark.read.csv('instacart/order_products__train.csv', header=True, sep=',', inferSchema=True)\n",
    "print('order_products__train.csv')\n",
    "df_order_prod.show(5)\n",
    "\n",
    "df_orders = spark.read.csv('instacart/orders.csv', header=True, sep=',', inferSchema=True)\n",
    "print('orders.csv')\n",
    "df_orders.show(5)\n",
    "\n",
    "df_products = spark.read.csv('instacart/products.csv', header=True, sep=',', inferSchema=True)\n",
    "print('products.csv')\n",
    "df_products.show(5)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "List of products ever purchased by each user\n",
    "\"\"\"\n",
    "# USING SQL\n",
    "df_order_prod.createOrReplaceTempView(\"order_prod\") # creates table 'order_prod'\n",
    "df_orders.createOrReplaceTempView(\"orders\") # creates table 'orders'\n",
    "df_products.createOrReplaceTempView(\"products\") # creates table 'products'\n",
    "spark.sql('SELECT o.user_id, COLLECT_LIST(p.product_name) AS products' \n",
    "               ' FROM orders o '\n",
    "               ' INNER JOIN order_prod op ON op.order_id = o.order_id'\n",
    "               ' INNER JOIN products p    ON op.product_id = p.product_id'\n",
    "               ' GROUP BY user_id ORDER BY o.user_id').show(5, truncate=80)\n",
    "\n",
    "\n",
    "# USING DATAFRAME OPERATIONS\n",
    "# df_orders.join(df_order_prod, df_order_prod.order_id == df_orders.order_id, 'inner')\\\n",
    "# .join(df_products, df_products.product_id == df_order_prod.product_id, 'inner')\\\n",
    "# .groupBy(df_orders.user_id).agg(f.collect_list(df_products.product_name).alias('products'))\\\n",
    "# .orderBy(df_orders.user_id).show(5, truncate=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEqVeqhkMa0a"
   },
   "source": [
    "### 3.1 Perspectives commerciales (25 points) \n",
    "\n",
    "Maintenant, vous √™tes le *data scientist*. En ne consid√©rant que les commandes de ``order_products__train.csv``, l'utilisation du module Spark SQL, performant avec SQL ou dataframe, pour r√©pondre aux questions suivantes:\n",
    "\n",
    "1. Quels sont les 10 produits les plus susceptibles d'√™tre command√© de nouveau? Ne consid√©rez que les produits achet√©s au moins 40 fois pour cette t√¢che.\n",
    "2. Quels sont les 3 produits les plus achet√©s dans chaque d√©partement?\n",
    "4. Quelle est la taille moyenne du panier pour chaque jour de la semaine?\n",
    "    - utilisez un barplot pour visualiser vos r√©sultats\n",
    "\n",
    "**La sortie de ces questions doit contenir le NOM des produits, pas leur ID.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEWqTH1QMa0a"
   },
   "source": [
    "### 3.2 MBA pour le training set (15 points)\n",
    "\n",
    "En utilisant les commandes du ``order_products__train.csv``, cr√©ez un bloc de donn√©es o√π chaque ligne contient la colonne ``transaction`` avec la liste des produits achet√©s, de mani√®re similaire √† le toy dataset. Ensuite, ex√©cutez l'algorithme MBA pour cet ensemble de transactions.\n",
    "\n",
    "- Vous devez signaler le temps pass√© pour effectuer cette t√¢che.\n",
    "- La sortie doit contenir le nom des produits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxZh_f3hMa0b"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "TODO: create a query to create and sctruct the transactions\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dewN0YUEMa0h"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "TODO: run the MBA algorithm and show the first 5 association rules\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DkNPEtGMa0l"
   },
   "source": [
    "# 3.3 MBA pour le dataset complet (15 points)\n",
    "\n",
    "Comme vous l'avez probablement remarqu√©, m√™me pour un ensemble de donn√©es moins volumineux (le training dataset ne contient que 131 000 commandes), l'algorithme MBA est co√ªteux en calcul. Pour cette raison, cette fois, nous allons r√©p√©ter le processus, mais en utilisant maintenant Amazon Web Services (AWS) pour cr√©er un grand cluster. Toutes les instructions pour cr√©er un cluster avec spark et comment soumettre un travail seront expliqu√©es dans le laboratoire. Dans tous les cas, vous devez lire les instructions donn√©es dans le ``Instruction_AWS.pdf``.\n",
    "\n",
    "Cette fois, nous travaillerons avec le fichier ``order_products__prior.csv``, qui contient plus de 3M commandes.\n",
    "\n",
    "**PRODUCTION ATTENDUE**\n",
    "\n",
    "Apr√®s avoir ex√©cut√© le MBA pour la plus grande collection de commandes, s√©lectionnez au hasard UN produit achet√© dans ``order_products__prior`` et affichez les r√®gles d'association (nom du produit et valeur d'association) de ce produit, c'est-√†-dire lorsque le produit est seul dans le panier. La sortie doit √™tre format√©e dans un tableau, o√π chaque ligne contenant les informations d'un produit associ√©. \n",
    "\n",
    "- Affichez l'ID et le nom du produit s√©lectionn√© au hasard.\n",
    "- Signaler le temps d'ex√©cution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1cVWxraMa0l"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "TODO: create a query to create and sctruct the transactions from the order_products__prior.csv file\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVBQ12b2Ma0o"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "TODO: run the MBA algorithm and print the requested output\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Bonus - Empreinte carbone (1 point)\n",
    "\n",
    "De nos jours, en apprentissage automatique, et en particulier pour le traitement du langage naturel (NLP), il est courant d'entra√Æner de grands r√©seaux de neurones sur des ensembles de donn√©es massifs. Par exemple, il faut 78 ans pour entra√Æner le mod√®le GPT-3, un r√©seau de neurones de pointe en NLP avec 175 milliards de param√®tres, en utilisant un seul GPU V100 \\([Wolff Anthony et al, 2020](https://arxiv.org/pdf/2007.03051.pdf)\\). L'√©mission de CO2 pour entra√Æner ce mod√®le √©quivaut √† conduire une voiture au Canada sur environ 550 000 km ! √âtant donn√© que ces environnements √† forte intensit√© √©nerg√©tique sont devenus populaires et se sont d√©velopp√©s ces derni√®res ann√©es, l'apprentissage automatique (ML) pourrait commencer √† devenir un acteur important du changement climatique.\n",
    " \n",
    "Il est maintenant temps de mesurer les √©missions de carbone g√©n√©r√©es dans ce TP sur la base de [Strubell et al 2019](https://arxiv.org/pdf/1906.02243.pdf). Nous supposons que les machines sont situ√©es aux √âtats-Unis et que les deux processeurs (2,3 GHz Intel Xeon¬Æ E5-2686 v4) et la DRAM (8 GB) consomment en moyenne 243 watts par heure.\n",
    "\n",
    "\n",
    "**Vous devez ex√©cuter la cellule ci-dessous avec le nombre d'heures pass√©es dans AWS pour recevoir le point.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def compute_co2_emission(x):\n",
    "    \"\"\"\n",
    "    x = number of hours\n",
    "    \"\"\"\n",
    "    kwh = (243.0 * x)/1000\n",
    "    #\n",
    "    co2lbs = 0.954 * (1.58 * kwh)\n",
    "    \n",
    "    display(HTML(f\"\"\"\n",
    "        <h4>In this TP, you generated {co2lbs} CO2e (lbs) which is equivalent to:  </h4>\n",
    "        </br>\n",
    "        <div>\n",
    "            <ul>\n",
    "              <li> {round(co2lbs/6.61,2)} servings of beef üêÑ (4 oz. meat per serving)</li>\n",
    "              <li> {round(co2lbs * 0.337307,2)} km  of driving a car üöó in Canada</li>\n",
    "            </ul> \n",
    "        </div>\n",
    "    \"\"\"))\n",
    "\n",
    "    \n",
    "hours = #TODO: Insert the number of hours spent in AWS\n",
    "compute_co2_emission(hours)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tp2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
